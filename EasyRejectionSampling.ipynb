{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "First imports/installs for this to run. As of Aug 1st, 2023, this will run in a high-RAM Google Colab instance"
      ],
      "metadata": {
        "id": "Mxd4Z4l7qqWV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIx--vFanx28"
      },
      "outputs": [],
      "source": [
        "!pip install jaxopt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Inqdx0tL5C8j"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OswpktPmx7ck"
      },
      "outputs": [],
      "source": [
        "!pip install optax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HXbC6ieTKmP"
      },
      "outputs": [],
      "source": [
        "# The sampling is better behaved in 64 bit mode\n",
        "from jax.config import config\n",
        "config.update(\"jax_enable_x64\", True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9I7vsthHRe6r"
      },
      "outputs": [],
      "source": [
        "import graphviz\n",
        "import os\n",
        "import arviz as az\n",
        "import xarray as xr\n",
        "from tqdm.autonotebook import tqdm\n",
        "# import tqdm\n",
        "\n",
        "import time\n",
        "\n",
        "from IPython.display import set_matplotlib_formats\n",
        "import jax.numpy as jnp\n",
        "from jax import random, vmap\n",
        "from jax.scipy.special import logsumexp, logit, expit\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "from google.colab import files\n",
        "import jax\n",
        "from jax.scipy import stats\n",
        "from jax.scipy.linalg import cholesky\n",
        "import sympy\n",
        "import sympy as sp\n",
        "from functools import partial\n",
        "from typing import Callable\n",
        "import scipy\n",
        "import matplotlib.tri as tri"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0FtSWGJXh_4"
      },
      "outputs": [],
      "source": [
        "from tensorflow_probability.substrates import jax as tfp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApFSZrZfTiDA"
      },
      "outputs": [],
      "source": [
        "import jaxopt\n",
        "from jaxopt import ProjectedGradient\n",
        "from jaxopt.projection import projection_box\n",
        "import optax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kui2eRp0iJvu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0I9oFgKUpLU"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "key = jax.random.PRNGKey(seed)\n",
        "key, subkey = jax.random.split(key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8llEaJsfWQzM"
      },
      "outputs": [],
      "source": [
        "#written by Enzo Michelangeli, style changes by josef-pktd\n",
        "# Student's T random variable\n",
        "@partial(jax.jit, static_argnums=(4,))\n",
        "def multivariate_t_rvs(key, m, S, df=10000.0, n=1):\n",
        "    '''generate random variables of multivariate t distribution\n",
        "    Parameters\n",
        "    ----------\n",
        "    m : array_like\n",
        "        mean of random variable, length determines dimension of random variable\n",
        "    S : array_like\n",
        "        square array of covariance  matrix\n",
        "    df : int or float\n",
        "        degrees of freedom\n",
        "    n : int\n",
        "        number of observations, return random array will be (n, len(m))\n",
        "    Returns\n",
        "    -------\n",
        "    rvs : ndarray, (n, len(m))\n",
        "        each row is an independent draw of a multivariate t distributed\n",
        "        random variable\n",
        "    '''\n",
        "    m = jnp.asarray(m)\n",
        "    d = m.shape[0]\n",
        "    x = jax.random.gamma(key, df/2, shape=(n,))/(df)\n",
        "    x = x.reshape(-1,1)\n",
        "    z = jax.random.multivariate_normal(key, jnp.zeros(d),S,(n,))\n",
        "    return m + z/jnp.sqrt(x)\n",
        "\n",
        "@partial(jax.jit)\n",
        "def mvStudent_logpdf(x, mean, shape, df):\n",
        "    dim = mean.shape[0]\n",
        "\n",
        "    vals, vecs = jnp.linalg.eigh(shape)\n",
        "    vals += 1e-7\n",
        "    logdet     = jnp.log(vals).sum()\n",
        "    valsinv    = 1./vals #jnp.array([1./v for v in vals])\n",
        "    U          = vecs * jnp.sqrt(valsinv)\n",
        "    dev        = x - mean\n",
        "    maha       = jnp.square(jnp.dot(dev, U)).sum(axis=-1)\n",
        "    #print(maha)\n",
        "\n",
        "    t = 0.5 * (df + dim)\n",
        "    A = jax.scipy.special.gammaln(t)\n",
        "    B = jax.scipy.special.gammaln(0.5 * df)\n",
        "    C = dim/2. * jnp.log(df * np.pi)\n",
        "    D = 0.5 * logdet\n",
        "    E = -t * jnp.log(1 + (1./df) * maha)\n",
        "\n",
        "    return A - B - C - D + E"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCvQh5RaEnlo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wmh-fTrIhaHW"
      },
      "outputs": [],
      "source": [
        "def fill_diagonal(a, val):\n",
        "  # https://github.com/google/jax/issues/2680\n",
        "  assert a.ndim >= 2\n",
        "  i, j = jnp.diag_indices(min(a.shape[-2:]))\n",
        "  return a.at[..., i, j].set(val)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEqE_6MTEo6J"
      },
      "source": [
        "# K-Means++ Seed Selection\n",
        "\n",
        "Used for the GMM fitting process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRbytCioEs5a"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def assign(x, means):\n",
        "  \"\"\"\n",
        "  x: shape (n, d)\n",
        "  means: shape (n, d)\n",
        "  \"\"\"\n",
        "  assignment = jax.vmap(\n",
        "      lambda point: jnp.argmin(jax.vmap(jnp.linalg.norm)(means - point))\n",
        "  )(x)\n",
        "  dists = jax.vmap(jnp.linalg.norm)(means[assignment,:] - x)\n",
        "  return assignment, dists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ja1eSgVvEuli"
      },
      "outputs": [],
      "source": [
        "@partial(jax.jit, static_argnums=(1,))\n",
        "def kpp_seeds(X, K, key, w=None):\n",
        "  \"\"\"\n",
        "  X: has shape (n, d) for the dataset\n",
        "  K: the number of seeds we want\n",
        "  key: is a PRNG key\n",
        "  w: the weights for the seed selection\n",
        "  return int32 tensor of shape (k) with the indicies of the seeds in X\n",
        "  \"\"\"\n",
        "  key, subkey = jax.random.split(key) #we are going to need k PRNG steps\n",
        "\n",
        "  n, d = X.shape\n",
        "  if w is None:\n",
        "    w = jnp.ones(n)/n\n",
        "\n",
        "  seeds = jnp.zeros(K, dtype=jnp.int32)\n",
        "\n",
        "\n",
        "  seeds = seeds.at[0].set(jax.random.randint(subkey, minval=0, maxval=n-1, shape=(1,))[0])\n",
        "  dists = jnp.linalg.norm(X-X[seeds[0],:], axis=-1)\n",
        "  assignment = jnp.zeros(n, dtype=jnp.int32)\n",
        "\n",
        "  def kpp_update(k_, val):\n",
        "    assignment, dists, seeds, key = val\n",
        "\n",
        "    #Lets select a new seed by weighted probability\n",
        "    key, subkey = jax.random.split(key)\n",
        "    d2 = w*dists*dists\n",
        "    prob_select = d2/d2.sum()\n",
        "    to_add = jax.random.choice(subkey, n, shape=(1,), p=prob_select)[0]\n",
        "    seeds = seeds.at[k_].set(to_add)\n",
        "\n",
        "    #Whats everyones distance to this?\n",
        "    new_mean = X[seeds[k_],:]\n",
        "    new_mean_dists = jnp.linalg.norm(X-new_mean, axis=-1)\n",
        "\n",
        "    #Lets update the stats\n",
        "    changed_ownership = new_mean_dists < dists\n",
        "    dists = jnp.where(changed_ownership, new_mean_dists, dists)\n",
        "    assignment = jnp.where(changed_ownership, k_, assignment)\n",
        "\n",
        "    return assignment, dists, seeds, key\n",
        "\n",
        "  init_values = assignment, dists, seeds, key\n",
        "\n",
        "  assignment, dists, seeds, key = jax.lax.fori_loop(1, K, kpp_update, init_values)\n",
        "  return assignment, dists, seeds, key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKsLAxfHE1kH"
      },
      "source": [
        "# GMM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yB33j6kBSan2"
      },
      "outputs": [],
      "source": [
        "def fit_gmm(key, data, K, w=None, student=False, tol=0.001, max_iters=400, fullCov=True):\n",
        "  d = data.shape[1]\n",
        "  n = data.shape[0]\n",
        "\n",
        "  if w is None:\n",
        "    W_ = jnp.ones(n)\n",
        "  else:\n",
        "    W_ = w\n",
        "\n",
        "  S_all = jnp.cov(data, rowvar=False, aweights=W_).reshape(d,d)\n",
        "  # print(S_all.shape)\n",
        "  S_all = fill_diagonal(S_all, jnp.maximum(jnp.diag(S_all), 1e-3))\n",
        "\n",
        "  def e_step(args):\n",
        "    m, s = args\n",
        "    # if student:\n",
        "    #   log_pdfs = mvStudent_logpdf(data, m, s, df=1)\n",
        "    # else:\n",
        "    #   log_pdfs = jax.scipy.stats.multivariate_normal.logpdf(data, m, s)\n",
        "    log_pdfs = jax.lax.cond(fullCov,\n",
        "      lambda z: jax.scipy.stats.multivariate_normal.logpdf(data, m, s), #True\n",
        "      lambda z: jax.scipy.stats.norm.logpdf(data, loc=m, scale=jnp.diag(s)**0.5).sum(axis=-1),\n",
        "      m\n",
        "      )\n",
        "    # log_pdfs = data[:,0]*m[0]\n",
        "    return log_pdfs\n",
        "\n",
        "  def m_step(args):\n",
        "    w = args*W_\n",
        "    cov = jnp.cov(data, rowvar=False, aweights=w).reshape(d,d)\n",
        "    cov = jnp.nan_to_num(cov, nan=S_all.max())\n",
        "    cov = fill_diagonal(cov, jnp.maximum(jnp.diag(cov), 1e-3))\n",
        "    return jnp.average(data, axis=0, weights=w), cov.reshape((d,d))\n",
        "\n",
        "  def m_step_diag(args):\n",
        "    w = args*W_\n",
        "    avg = jnp.average(data, axis=0, weights=w)\n",
        "    # cov = jnp.cov(data, rowvar=False, aweights=w).reshape(d,d)\n",
        "    cov = jnp.average((data-avg.reshape(1, -1))**2, axis=0, weights=w)\n",
        "    cov = jnp.nan_to_num(cov, nan=S_all.max())\n",
        "    cov = jnp.maximum(cov, 1e-5)\n",
        "    return avg, jnp.diag(cov)\n",
        "\n",
        "  @partial(jax.jit, static_argnames=())\n",
        "  def whole_step(ùúá, Œ£, assignments):\n",
        "    log_probs = jax.vmap(e_step)((ùúá, Œ£))\n",
        "    weights = jax.nn.softmax(log_probs, axis=0)\n",
        "    # ùúá, Œ£ = jax.vmap(m_step)(weights)\n",
        "    ùúá, Œ£ = jax.lax.cond(fullCov,\n",
        "      lambda weights: jax.vmap(m_step)(weights), #True\n",
        "      lambda weights: jax.vmap(m_step_diag)(weights),\n",
        "      weights\n",
        "      )\n",
        "\n",
        "    new_assignments = jnp.argmax(log_probs, axis=0)\n",
        "    changes = (assignments != new_assignments).sum()\n",
        "    # assignments = new_assignments\n",
        "    return ùúá, Œ£, new_assignments, changes\n",
        "\n",
        "  assignments = jax.random.randint(key, shape=(n,), minval=0, maxval=K, dtype=jnp.int32)\n",
        "  #ùúá = jax.random.choice(key, data, shape=(K,), replace=False)# init random centers jnp.zeros((K,d))\n",
        "  assignments, dists, seeds, *_ = kpp_seeds(data, K, key, w=W_)# init random centers jnp.zeros((K,d))\n",
        "  weights = jax.nn.one_hot(assignments, K).T\n",
        "  # print(weights.shape, data.shape)\n",
        "  ùúá, Œ£ = jax.vmap(m_step)(weights)\n",
        "\n",
        "\n",
        "  changes = n\n",
        "  iters = 0\n",
        "  # while changes > n*tol and iters < max_iters:\n",
        "  with tqdm(total=max_iters, leave=False) as pbar:\n",
        "    while changes >= n*tol and iters < max_iters:\n",
        "      ùúá, Œ£, assignments, changes = whole_step(ùúá, Œ£, assignments)\n",
        "      iters += 1\n",
        "      pbar.set_description(f\"changes {changes/float(n)}%\")\n",
        "    # print(ùúá.T)\n",
        "      pbar.update(1)\n",
        "    pbar.update(max_iters-iters)\n",
        "\n",
        "    # print(\"\\t\", changes)\n",
        "  return ùúá, Œ£, jax.nn.softmax(jax.vmap(e_step)((ùúá, Œ£)), axis=0).sum(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ho7z30LqM6y"
      },
      "outputs": [],
      "source": [
        "@partial(jax.jit)\n",
        "def mm_log_pdf(data, ùúá, Œ£, weights, student=False, df=1e10, fullCov=True):\n",
        "  \"\"\"\n",
        "  ùúá must be shaped as (clusters, d)\n",
        "  Œ£ must be shaped as (clusters, d, d)\n",
        "  \"\"\"\n",
        "  d = ùúá.shape[1]\n",
        "\n",
        "  data = data.reshape(-1, d)\n",
        "  def LL(args):\n",
        "    m, s = args\n",
        "    log_pdfs = jax.lax.cond(student,\n",
        "      lambda x: mvStudent_logpdf(x, m, s, df=df), #True\n",
        "      lambda x: jax.lax.cond(fullCov,\n",
        "        lambda z: jax.scipy.stats.multivariate_normal.logpdf(x, m, s),  #True\n",
        "        lambda z: jax.scipy.stats.norm.logpdf(x, loc=m, scale=jnp.diag(s)**0.5).sum(axis=-1),\n",
        "        m ),\n",
        "      data\n",
        "      ) #False\n",
        "    # log_pdfs = data[:,0]*m[0]\n",
        "    return log_pdfs\n",
        "  weights = weights / weights.sum()\n",
        "  weighted_log_pdfs = jax.vmap(LL)((ùúá, Œ£)) + jnp.log(weights+1e-7).reshape(-1, 1)\n",
        "  return jax.scipy.special.logsumexp(weighted_log_pdfs, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to draw samples from our GMM using the bounds provided. Also has code support for using the full covariance and a naive rejection approach, but it is extremly slow if using bounded support. So we advice against it."
      ],
      "metadata": {
        "id": "S4m0wrdIsUA5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSH-SjNqasIQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def getSamples(key, n, ùúá, Œ£, weights=None, low_bounds=None, hi_bounds=None, student=False, df=None, fullCov=True):\n",
        "\n",
        "  if low_bounds is None:\n",
        "    low_bounds = jax.lax.full_like(ùúá[0,:], -jnp.inf)\n",
        "  if hi_bounds is None:\n",
        "    hi_bounds = jax.lax.full_like(ùúá[0,:], jnp.inf)\n",
        "  if weights is None:\n",
        "    weights = jax.lax.full_like(ùúá[:,0], 1.0)\n",
        "\n",
        "  samples_per_cluster = tfp.distributions.Multinomial(n, probs=weights/weights.sum()).sample(sample_shape=(1), seed=key)[0,:].astype(jnp.int32)\n",
        "\n",
        "  key, subkey = jax.random.split(key)\n",
        "\n",
        "  d = ùúá.shape[1]\n",
        "  if df is None:\n",
        "    df = 1\n",
        "  all_samples = []\n",
        "  for i in range(ùúá.shape[0]):\n",
        "    cur_samples = jnp.zeros((0,d))\n",
        "\n",
        "    key, subkey = jax.random.split(key)\n",
        "    sample_efficency = 1.0 #How many extra samples do we need to get where we want to be?\n",
        "    while cur_samples.shape[0] < samples_per_cluster[i]:\n",
        "      samples_needed = samples_per_cluster[i]-cur_samples.shape[0]\n",
        "      n_ = jnp.round(sample_efficency*samples_needed).astype(jnp.int32)\n",
        "      if student:\n",
        "        new_samples = multivariate_t_rvs(subkey, ùúá[i,:], Œ£[i,:], df=df, n=n_)\n",
        "      else:\n",
        "        if fullCov:\n",
        "          new_samples = jax.random.multivariate_normal(subkey, ùúá[i,:], Œ£[i,:], (n_,))\n",
        "        else:\n",
        "          m = ùúá[i,:]\n",
        "          s = jnp.diag(Œ£[i,:])**0.5\n",
        "          new_samples = jax.random.truncated_normal(subkey, (low_bounds-m)/s, (hi_bounds-m)/s, shape=(n_,d))*s+m\n",
        "\n",
        "      #find out-of-bound samples\n",
        "      bad_samples = jnp.logical_or(jnp.logical_or(new_samples < low_bounds, new_samples > hi_bounds), jnp.isnan(new_samples)).sum(axis=1)\n",
        "      new_samples = new_samples[bad_samples == 0, :]\n",
        "      # print(\"Bad Samples: \", bad_samples.sum())\n",
        "      sample_efficency = jnp.maximum(samples_needed/(new_samples.shape[0]+1), sample_efficency)\n",
        "      #add to the total set of samples\n",
        "      cur_samples = jnp.vstack([cur_samples, new_samples])\n",
        "\n",
        "      key, subkey = jax.random.split(key)\n",
        "\n",
        "    all_samples.append(cur_samples)\n",
        "  return jnp.vstack(all_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hobs2BJUV67F"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def norm_invcdf(p):\n",
        "  p = jnp.clip(p, 1e-16, 1-1e-16)\n",
        "  return jnp.sqrt(2)*jax.scipy.special.erfinv(2*p-1)\n",
        "\n",
        "def rnd_trunk_normal(key, l, h, m, s, n):\n",
        "  a, b = (l-m)/s, (h-m)/s\n",
        "  phi_a =  jax.scipy.stats.norm.cdf(a)\n",
        "  phi_b =  jax.scipy.stats.norm.cdf(b)\n",
        "  U = jax.random.uniform(key, (n, m.shape[-1]))\n",
        "  return norm_invcdf(phi_a + U*(phi_b-phi_a))*s+m\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvQ3tOSbKZK-"
      },
      "source": [
        "# Target Functions\n",
        "\n",
        "These are the functions from the paper that we use as benchmarks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWSv1MyRzTZz"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def log_f_Erraqabi(x):\n",
        "  # f_ = lambda z: jnp.log(jnp.sin(4*jnp.pi*z-jnp.pi/2)+1)\n",
        "  # return jax.vmap(f_)(x).sum(axis=-1)\n",
        "  z = jnp.sin(4*jnp.pi*x-jnp.pi/2)+1\n",
        "  z = jnp.nan_to_num(jnp.log(z), nan=-1e38)\n",
        "  return jnp.nan_to_num(z.sum(axis=-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o63fbaC42P_O"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def log_f_Maddison(x, a):\n",
        "  z = (-x - a*jnp.log(1+jnp.maximum(x, 0.0))).sum(axis=-1, keepdims=True)\n",
        "  return jnp.where(x >= 0, z, -1e38).reshape(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjWOVKtTvdft"
      },
      "outputs": [],
      "source": [
        "@partial(jax.jit, static_argnums=())\n",
        "def log_f_clutter(x, data, sigma=2, pi=0.5):\n",
        "  dim = data.shape[1]\n",
        "  n = x.shape[0]\n",
        "  x = x.reshape(n, -1)\n",
        "  log_prior = -0.5*(x ** 2/sigma**2).sum(axis=-1) - dim*0.5*jnp.log(2*jnp.pi) - jnp.log(sigma).sum()\n",
        "\n",
        "  model = (-0.5*((data[jnp.newaxis,:,:]-x[:,jnp.newaxis,:]) ** 2).sum(axis=-1) - dim * 0.5 * jnp.log(2*jnp.pi))\n",
        "  noise = -0.5*(data ** 2 / 100. ** 2).sum(axis=-1) - dim * 0.5 * jnp.log(2*jnp.pi) - dim * jnp.log(100)\n",
        "  noise = jnp.repeat(noise.reshape(-1, data.shape[0]), model.shape[0], axis=0)\n",
        "  log_likelihood = logsumexp(jnp.stack([model + jnp.log(pi), noise + jnp.log(1-pi)], axis=-1), axis=-1).sum(axis=-1)\n",
        "  negative_energy = log_likelihood + log_prior\n",
        "  return negative_energy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk4SpGn6L0d7"
      },
      "source": [
        "# Easy Rejection Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWtVeeMvW6WN"
      },
      "outputs": [],
      "source": [
        "def getSubsetIDs(key, s, to_grab): #Not effectivly used in current code, we always pick all of them\n",
        "  if to_grab >= s.shape[0]:\n",
        "    return jnp.arange(s.shape[0])\n",
        "  to_grab = to_grab//3\n",
        "  # jnp.argsort(-s)[0:to_grab]\n",
        "  keya, subkey = jax.random.split(key)\n",
        "  stochastic_highs = jax.random.choice(subkey, int(s.shape[0]), shape=(int(to_grab),), p=jax.nn.softmax(s))\n",
        "  keya, subkey = jax.random.split(key)\n",
        "  stochastic_others = jax.random.choice(subkey, int(s.shape[0]), shape=(int(to_grab),))\n",
        "  lowest = jnp.arange(s.shape[0])[jnp.argsort(-s)[0:to_grab]]\n",
        "  ids = jnp.unique(jnp.hstack((stochastic_highs, stochastic_others, lowest)))\n",
        "  return ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hLJuqWjscWb"
      },
      "outputs": [],
      "source": [
        "def ers(key, log_func, d, target_samples=10000, low_val=-jnp.inf, hi_val=jnp.inf, samples_at_a_time = 500):\n",
        "  \"\"\"\n",
        "  log_f: the log PDF of the function to draw samples from\n",
        "  d: the number of dimensions to be sampling\n",
        "  \"\"\"\n",
        "  key, subkey = jax.random.split(key)\n",
        "  use_heavy_tail = False\n",
        "\n",
        "  history = {}\n",
        "  history[\"logC\"] = []\n",
        "  history[\"rate\"] = []\n",
        "  history[\"samples\"] = []\n",
        "  history[\"fits\"] = []\n",
        "\n",
        "  fullCov = False # d < 4\n",
        "\n",
        "  f_eval_total = 0 #Keep track of how many times f has been evaluated\n",
        "\n",
        "  low_box = jnp.ones(d)*low_val\n",
        "  hi_box = jnp.ones(d)*hi_val\n",
        "\n",
        "  gmm_time = 0.0\n",
        "  refine_time = 0.0\n",
        "  init_time = 0.0\n",
        "  sampling_time = 0.0\n",
        "\n",
        "  start = time.time()\n",
        "  w_init = jax.random.uniform(subkey, (d,), minval=low_box, maxval=hi_box)\n",
        "  w_init = jnp.nan_to_num(w_init, neginf=0.5, posinf=0.5) #if a box was +- inf, this will set it to zero\n",
        "  step_size_search = 0.0 #By default we will use a line search indicate by 0\n",
        "\n",
        "  if jnp.isfinite(low_box).all() and jnp.isfinite(hi_box).all():\n",
        "    # print(\"Using box center as initial mean\")\n",
        "    f_eval_total += 1\n",
        "    w_init = hi_box-low_box\n",
        "    f_mode = w_init/2\n",
        "    Sigma_est = jnp.diag(w_init/3) #Should cover the whole box\n",
        "    ùúá = f_mode.reshape(1, d) #shape with (1, *) b/c we will treat it as a GMM for code simplicity, later one we add more means\n",
        "    Œ£ = Sigma_est.reshape(1, d, d)\n",
        "    weights = jnp.ones((1))\n",
        "  else:\n",
        "    while log_func(w_init).max() < -1e36: #If discontinous we could by chance start in a bad spod\n",
        "      # print(\"Attempting to sample a good point, b/c last one was \", log_func(w_init).max())\n",
        "      key, subkey = jax.random.split(key)\n",
        "      f_eval_total += 1\n",
        "      step_size_search = 1e-4 # Line searches are a bad idea with discontinuities, lets use normal SGD\n",
        "      w_init = jax.random.uniform(subkey, (d,), minval=low_box, maxval=hi_box)\n",
        "      w_init = jnp.nan_to_num(w_init, neginf=0.5, posinf=0.5) #if a box was +- inf, this will set it to zero\n",
        "\n",
        "    #First we find the mode of the target distribution\n",
        "    if  jnp.isfinite(low_box).all() and jnp.isfinite(hi_box).all():\n",
        "      searchers = jnp.concatenate([w_init.reshape(1, d), jax.random.uniform(subkey, (2**d+1,d), minval=low_val, maxval=hi_val)], axis=0)\n",
        "    else:\n",
        "      searchers = jnp.concatenate([w_init.reshape(1, d), w_init+jax.random.normal(subkey, (2**d+d+1,d))], axis=0)\n",
        "    searchers = jnp.clip(searchers, low_box+1e-2, hi_box-1e-2)\n",
        "    key, subkey = jax.random.split(key)\n",
        "    pg = ProjectedGradient(fun=lambda x: -log_func(x).sum(), projection=projection_box, stepsize=step_size_search, jit=True, maxiter=50, maxls=15, tol=0.01)\n",
        "    pg_sol_mode = pg.run(searchers, hyperparams_proj=(low_box+1e-2, hi_box-1e-2))\n",
        "    f_mode = pg_sol_mode.params\n",
        "    f_eval_total += pg_sol_mode.state.iter_num*15*searchers.shape[0]\n",
        "    f_mode = jnp.clip(f_mode, low_box+1e-2, hi_box-1e-2)\n",
        "    f_high_logpdf = log_func(f_mode)\n",
        "\n",
        "    if jnp.cov(f_mode, rowvar=False).max() < 0.01: #Looks unimodal, we need to figure out a decent stnd. dev.\n",
        "      # print(\"Unimodal case\")\n",
        "      f_mode = f_mode[0,:]\n",
        "\n",
        "      #Now we find a set of \"seed\" points that are use to estimate a covariance of the\n",
        "      #  distribution. The seeds are started from randomly perturbed mode, and\n",
        "      # optimized to have a lower PDF to encourage them to go aware from the mode\n",
        "      initial_spread = f_mode + jax.random.normal(key, (d*2+10, d))\n",
        "      initial_spread = jnp.clip(initial_spread, low_box+1e-2, hi_box-1e-2)\n",
        "\n",
        "      def spreadLoss(centers, spread, shift=-5):\n",
        "        f_main = log_func(centers)\n",
        "        f_spread = log_func(spread)\n",
        "        closest =  (f_main.flatten() - f_spread.reshape(-1, 1)).min(axis=-1) #what was closest\n",
        "        return jnp.mean(jnp.power((closest+shift), 2))\n",
        "\n",
        "      pg = ProjectedGradient(fun=lambda z: spreadLoss(f_mode, z, -5), projection=projection_box, jit=True, stepsize=0.0, maxiter=10, maxls=5, tol=0.01)\n",
        "      pg_sol = pg.run(initial_spread, hyperparams_proj=(low_box+1e-2, hi_box-1e-2))\n",
        "      f_eval_total += pg_sol.state.iter_num*10*initial_spread.shape[0]\n",
        "\n",
        "      seed_points = jnp.vstack((f_mode, pg_sol.params))\n",
        "      logPDF_seeds = log_func(seed_points)\n",
        "      weights = jax.nn.softmax(logPDF_seeds.flatten())\n",
        "      Sigma_est = jnp.cov(seed_points, rowvar=False, aweights=weights) + jnp.diag(jnp.ones(d))*0.1\n",
        "      #We have now chosen an initial starting point w_init, as well as an initial covariance Sigma_est, to use as our initial functoin g(x)\n",
        "      ùúá = f_mode.reshape(1, d) #shape with (1, *) b/c we will treat it as a GMM for code simplicity, later one we add more means\n",
        "      Œ£ = Sigma_est.reshape(1, d, d)\n",
        "      weights = jnp.ones((1))\n",
        "    else: #We found more than one mode, lets just use those.\n",
        "      # print(\"Multi-modal case\")\n",
        "      modes = f_mode\n",
        "      pair_dist = jnp.sqrt(jnp.sum((modes[:, None, :] - modes[None, :, :])**2, axis=-1))\n",
        "      i, j = jnp.unravel_index(jnp.argmax(pair_dist, axis=None), pair_dist.shape)\n",
        "      max_dist = pair_dist[i,j]\n",
        "\n",
        "      selected_modes = jnp.concatenate([modes[i:i+1,:], modes[j:j+1,:]], axis=0)\n",
        "      pair_dist = jnp.sqrt(jnp.sum((selected_modes[:, None, :] - modes[None, :, :])**2, axis=-1)).min(axis=0)\n",
        "      next_fathest = jnp.argmax(pair_dist)\n",
        "      while pair_dist[next_fathest] > 0.01: #We have another valid mode\n",
        "        selected_modes = jnp.concatenate([selected_modes, modes[next_fathest:next_fathest+1,:]], axis=0)\n",
        "        pair_dist = jnp.sqrt(jnp.sum((selected_modes[:, None, :] - modes[None, :, :])**2, axis=-1)).min(axis=0)\n",
        "        next_fathest = jnp.argmax(pair_dist)\n",
        "      #We place a gaussian mode at each mode and use a shrunk Œ£ based on pairwise mode cov\n",
        "      ùúá = selected_modes #shape with (1, *) b/c we will treat it as a GMM for code simplicity, later one we add more means\n",
        "      # Sigma_est = jnp.cov(selected_modes, rowvar=False)*0.5+0.5*jnp.eye(d)\n",
        "      if selected_modes.shape == 2:\n",
        "        Sigma_est = jnp.eye(d)*max_dist\n",
        "      else:\n",
        "\n",
        "        pair_dist = jnp.sqrt(jnp.sum((selected_modes[:, None, :] - selected_modes[None, :, :])**2, axis=-1))\n",
        "        v = fill_diagonal(pair_dist, 1e20).min(axis=0).max()\n",
        "        v = v/(selected_modes.shape[0]*3)\n",
        "        Sigma_est = jnp.eye(d)*v\n",
        "      Œ£ = jnp.repeat(Sigma_est.reshape((1, d, d)), selected_modes.shape[0], axis=0)\n",
        "      weights = jnp.ones((selected_modes.shape[0]))/selected_modes.shape[0]\n",
        "\n",
        "  init_time += time.time() - start\n",
        "  history[\"init_g\"] = (ùúá, Œ£)\n",
        "  #Now we have initial sigma and mu\n",
        "  # print(ùúá.shape)\n",
        "  # print((ùúá, Œ£))\n",
        "  total_sampled = 0\n",
        "  total_accepted =0\n",
        "  log_C = -1e30\n",
        "\n",
        "  cur_samples = []\n",
        "  cur_samples_f = []\n",
        "\n",
        "  rej_samples = []\n",
        "  rej_samples_f = []\n",
        "\n",
        "  max_dev = 20#jnp.minimum((hi_box - low_box).max()*3, 20)-1e-2\n",
        "  refit_cycle = 2 #How many X times more data before we refit/refine the model?\n",
        "\n",
        "  keya = key\n",
        "  keya, subkey = jax.random.split(keya)\n",
        "  df = 1000.0#np.inf\n",
        "  ignore_threshold = -1e36\n",
        "\n",
        "  ùúá, Œ£, weights = ùúá.astype(jnp.float64), Œ£.astype(jnp.float64), weights.astype(jnp.float64)\n",
        "  K_orig = ùúá.shape[0]\n",
        "  last_gmm_fit_size = 10\n",
        "  refit = True\n",
        "  last_fit_sample_size = 5\n",
        "  #used for 'undoing' a bad GMM attempt\n",
        "  logC_old = 1e20\n",
        "  ùúá_old, Œ£_old = ùúá, Œ£\n",
        "  gmm_ban = 0\n",
        "  gmm_ban_strength = 2\n",
        "  gmm_size_factor = 1\n",
        "\n",
        "  log_C_best = 1000.0\n",
        "\n",
        "  regression = False\n",
        "  prev_rate = 0.0\n",
        "  # print(\"Starting \" , ùúá, Œ£)\n",
        "  first_iter = True\n",
        "  with tqdm(total=target_samples, leave=False) as pbar:\n",
        "    while total_accepted < target_samples:\n",
        "      pbar.set_description(\"Sampling at \"+str(prev_rate))\n",
        "      pbar.refresh()\n",
        "      start = time.time()\n",
        "      to_grab = int(samples_at_a_time*ùúá.shape[0])\n",
        "      # to_grab = int(samples_at_a_time*jnp.log(ùúá.shape[0]+1))\n",
        "      candidates = getSamples(subkey, to_grab, ùúá, Œ£, weights, low_bounds=low_box, hi_bounds=hi_box, fullCov=fullCov)\n",
        "      keya, subkey = jax.random.split(keya)\n",
        "      total_sampled += candidates.shape[0]\n",
        "      gmm_ban -= 1\n",
        "\n",
        "      f_eval_total += candidates.shape[0]\n",
        "      log_f = log_func(candidates)\n",
        "      valid = log_f > ignore_threshold #Candidates that occured in a zero area of the target function\n",
        "      # candidates = candidates[valid,:]\n",
        "      # log_f = log_f[valid]\n",
        "      #now sample\n",
        "      log_g = mm_log_pdf(candidates, ùúá, Œ£, weights, fullCov=fullCov)\n",
        "      #We now add a constant to g(x) so that g(x) >= f(x), but done in log-space\n",
        "\n",
        "      diff = log_f-log_g\n",
        "      log_C_hat = diff[valid].max()\n",
        "      log_C_best = min(log_C_best*(1+jnp.sign(log_C_best)*0.05), log_C_hat)\n",
        "      if log_C_hat - log_C_best > 0.05:\n",
        "        refit = True\n",
        "      if log_C_hat - log_C > 0.1 and total_accepted > 10: #Big change, lets adjust the model\n",
        "        refit = True\n",
        "        regression = log_C_hat - log_C > 0.2\n",
        "      else:\n",
        "        regression = False\n",
        "      log_C = logC_old = jnp.maximum(log_C_hat, log_C)\n",
        "      U = jnp.log(jax.random.uniform(subkey, (candidates.shape[0],)))\n",
        "      keya, subkey = jax.random.split(keya)\n",
        "\n",
        "      to_accept = U <= diff-log_C\n",
        "      cur_samples.append(candidates[to_accept,:])\n",
        "      cur_samples_f.append(log_f[to_accept].ravel())\n",
        "      rej_samples.append(candidates[~to_accept,:])\n",
        "      rej_samples_f.append(log_f[~to_accept].ravel())\n",
        "      total_accepted += to_accept.sum()\n",
        "\n",
        "      sampling_time += time.time() - start\n",
        "      prev_rate = float(to_accept.sum()/float(valid.shape[0]))\n",
        "      history[\"logC\"] += [float(log_C)]\n",
        "      history[\"rate\"] += [prev_rate]\n",
        "      history[\"samples\"] += [int(valid.shape[0])]\n",
        "\n",
        "      sizeToK = lambda x : jnp.minimum(jnp.log(x+1)/jnp.log(2), x/(d*15))\n",
        "\n",
        "      gmm_try = False\n",
        "      refine_try = False\n",
        "      refine_accpt = False\n",
        "\n",
        "      pbar.update(int(to_accept.sum()))\n",
        "      ùúá_old, Œ£_old, weights_old, log_C_old = ùúá, Œ£, weights, log_C\n",
        "      ùúá_pre, Œ£_pre, weights_pre = ùúá, Œ£, weights\n",
        "\n",
        "      if ((last_gmm_fit_size*1.5 < total_accepted) and total_accepted >= d**2 and gmm_ban < 0) or regression:\n",
        "        gmm_try = True\n",
        "        start = time.time()\n",
        "        pbar.set_description(\"Fitting new GMM\")\n",
        "        pbar.refresh()\n",
        "        #Compress everything into one tensor\n",
        "        last_gmm_fit_size = total_accepted\n",
        "        cur_samples = [jnp.vstack(cur_samples)]\n",
        "        cur_samples_f =  [jnp.hstack(cur_samples_f)]\n",
        "        rej_samples = [jnp.vstack(rej_samples)]\n",
        "        rej_samples_f =  [jnp.hstack(rej_samples_f)]\n",
        "        refit = True\n",
        "\n",
        "        K = jnp.round(sizeToK(total_accepted)).astype(jnp.int32)\n",
        "\n",
        "        samples = jnp.concatenate([cur_samples[0], rej_samples[0] ], axis=0)\n",
        "        w = jax.nn.softmax(jnp.hstack([cur_samples_f[0], rej_samples_f[0] ]))\n",
        "        w = w * jnp.hstack([jnp.ones_like(cur_samples_f[0])*10, jnp.ones_like(rej_samples_f[0])])\n",
        "        gmm_size_factor = 1\n",
        "        ùúá, Œ£, weights = fit_gmm(subkey, samples, int(K+K_orig), w=w, tol=1e-5, fullCov=fullCov)\n",
        "        keya, subkey = jax.random.split(keya)\n",
        "        # break\n",
        "\n",
        "        fs = cur_samples_f[0]\n",
        "        gs = mm_log_pdf(cur_samples[0], ùúá, Œ£, weights, fullCov=fullCov)\n",
        "        log_C = -1e30\n",
        "        gmm_time += time.time() - start\n",
        "\n",
        "      if total_accepted > max(last_fit_sample_size*refit_cycle, d*5):\n",
        "        refit = True\n",
        "\n",
        "      start = time.time()\n",
        "      if refit:\n",
        "        refine_try = True\n",
        "\n",
        "        cur_samples = [jnp.vstack(cur_samples)]\n",
        "        cur_samples_f =  [jnp.hstack(cur_samples_f)]\n",
        "        rej_samples = [jnp.vstack(rej_samples)]\n",
        "        rej_samples_f =  [jnp.hstack(rej_samples_f)]\n",
        "\n",
        "        # print(\"Adjusting the model to rejected samples\")\n",
        "        pbar.set_description(\"Adjusting the model to rejected samples\")\n",
        "        pbar.refresh() # to show immediately the update\n",
        "\n",
        "        if fullCov:\n",
        "          def safeSigma(s):\n",
        "            s = jnp.nan_to_num(s)\n",
        "            S = s @ s.T\n",
        "            diag = jnp.diag(S)\n",
        "            S = fill_diagonal(S, jnp.maximum(diag, 1e-5))\n",
        "            return S\n",
        "        else:\n",
        "           def safeSigma(s):\n",
        "             return jnp.diag(jnp.exp(s))\n",
        "\n",
        "        def optGMM(ùúá, Œ£, weights, log_C_old):\n",
        "          if fullCov:\n",
        "            Œ£_opt = jax.vmap(lambda X : jnp.linalg.cholesky(X))(Œ£)\n",
        "          else:\n",
        "            Œ£_opt = jax.vmap(lambda X : jnp.log(jnp.diag(X)+1e-16))(Œ£)\n",
        "\n",
        "\n",
        "          g_c = mm_log_pdf(cur_samples[0].reshape(-1, d), ùúá, Œ£, weights, fullCov=fullCov)\n",
        "          g_r = mm_log_pdf(rej_samples[0].reshape(-1, d), ùúá, Œ£, weights, fullCov=fullCov)\n",
        "\n",
        "          to_select_size = 10000000\n",
        "          cur_IDs = getSubsetIDs(subkey, cur_samples_f[0]-g_c, to_select_size)\n",
        "          rej_IDs = getSubsetIDs(subkey, rej_samples_f[0]-g_r, to_select_size)\n",
        "\n",
        "\n",
        "          cur_s_f = cur_samples_f[0][cur_IDs]\n",
        "          cur_s   = cur_samples[0][cur_IDs]\n",
        "          rej_s_f = rej_samples_f[0][rej_IDs]\n",
        "          rej_s   = rej_samples[0][rej_IDs]\n",
        "\n",
        "\n",
        "          #Refinmenet based on accepted & rejected samples\n",
        "          @jax.jit\n",
        "          def lossDiff(args):\n",
        "            m, S, w = args\n",
        "            S = jax.vmap(safeSigma)(S)\n",
        "            m = jnp.clip(m, low_val, hi_val)\n",
        "            w = jnp.nan_to_num(jax.nn.softmax(w), nan=0, neginf=0)\n",
        "            rej_g = mm_log_pdf(rej_s.reshape(-1, m.shape[-1]), m, S, w, fullCov=fullCov)\n",
        "            cur_g = mm_log_pdf(cur_s.reshape(-1, m.shape[-1]), m, S, w, fullCov=fullCov)\n",
        "\n",
        "            diff_rej =  rej_s_f-rej_g\n",
        "            diff_cur =  cur_s_f-cur_g\n",
        "            diff_a = jnp.hstack([diff_rej, diff_cur ])\n",
        "            #Weights of each item\n",
        "            diff_a_w = jax.nn.softmax(diff_a, axis=0)\n",
        "            loss = jnp.sum(diff_a_w * diff_a)\n",
        "            return jnp.nan_to_num(loss, nan=1000.0)\n",
        "\n",
        "          ùúá_opt = ùúá\n",
        "          w_opt = jnp.log(weights)\n",
        "          logC_new = 1e30\n",
        "          opt = optax.adabelief(0.1)\n",
        "          pgLD = jaxopt.OptaxSolver(opt=opt, fun=lossDiff, maxiter=800)\n",
        "          params = (ùúá_opt, Œ£_opt, w_opt)\n",
        "          state = pgLD.init_state(params)\n",
        "\n",
        "          best_sol = ùúá, Œ£, weights, log_C_old+1e-16 #No improvment\n",
        "\n",
        "          for steps in [100, 100, 200, 400]:\n",
        "            pbar.set_description(\"Adjusting the model to rejected samples, trying \" + str(steps))\n",
        "            pbar.refresh() # to show immediately the update\n",
        "\n",
        "            for _ in tqdm(range(steps), desc='Opt', leave=False):\n",
        "              params, state = pgLD.update(params, state)\n",
        "\n",
        "            ùúá2, Œ£2, w2 = params\n",
        "            ùúá_opt, Œ£_opt, w_opt = ùúá2, Œ£2, w2\n",
        "\n",
        "            if jnp.isnan(ùúá2).any() or jnp.isnan(w2).any():\n",
        "              # print(\"\\tOpt NaN\")\n",
        "              break\n",
        "\n",
        "            Œ£2 = jax.vmap(safeSigma)(Œ£2)\n",
        "            ùúá2 = jnp.clip(ùúá2, low_val, hi_val)\n",
        "            w2 = jax.nn.softmax(w2)\n",
        "\n",
        "\n",
        "            rej_g = mm_log_pdf(rej_samples[0].reshape(-1, d), ùúá2, Œ£2, w2, fullCov=fullCov)\n",
        "            cur_g = mm_log_pdf(cur_samples[0].reshape(-1, d), ùúá2, Œ£2, w2, fullCov=fullCov)\n",
        "            diff_rej =  rej_samples_f[0]-rej_g\n",
        "            diff_cur =  cur_samples_f[0]-cur_g\n",
        "            logC_new = jnp.maximum(diff_rej.max(), diff_cur.max())\n",
        "            if jnp.isnan(logC_new) :\n",
        "              break\n",
        "            #evaluate g(x) on the values we already have f(x) from this batch to see if we have improved the bound\n",
        "            if logC_new < best_sol[3]: #log_C_old - jnp.sign(log_C_old)/100:\n",
        "              log_C_old = logC_new\n",
        "\n",
        "              best_sol = ùúá2, Œ£2, w2, logC_new #We did it, we improved the model\n",
        "          return best_sol #No better solution found\n",
        "\n",
        "        if gmm_try:\n",
        "          ùúá_, Œ£_, weights_, newC = optGMM(ùúá, Œ£, weights, log_C_old)\n",
        "          if newC < log_C_old:\n",
        "            refine_accpt = True\n",
        "            log_C_old = newC\n",
        "            ùúá, Œ£, weights, log_C_old = ùúá_, Œ£_, weights_, newC\n",
        "        if refit: #gmm failed or no GMM round\n",
        "          ùúá_, Œ£_, weights_, newC = optGMM(ùúá_pre, Œ£_pre, weights_pre, log_C_old)\n",
        "        if newC < log_C_old:\n",
        "          ùúá, Œ£, weights, log_C_old = ùúá_, Œ£_, weights_, newC\n",
        "          refine_accpt = True\n",
        "        if not refine_accpt:\n",
        "          ùúá, Œ£, weights, log_C_old = ùúá_old, Œ£_old, weights_old, log_C_old\n",
        "        log_C = log_C_old\n",
        "\n",
        "        refit = False\n",
        "        last_fit_sample_size = total_accepted\n",
        "      refine_time += time.time() - start\n",
        "\n",
        "      keya, subkey = jax.random.split(keya)\n",
        "      history[\"fits\"] += [(gmm_try, refine_try, refine_accpt)]\n",
        "      # break\n",
        "  cur_samples = [jnp.vstack(cur_samples)]\n",
        "  cur_samples_f =  [jnp.hstack(cur_samples_f)]\n",
        "  rej_samples = [jnp.vstack(rej_samples)]\n",
        "  rej_samples_f =  [jnp.hstack(rej_samples_f)]\n",
        "  history['time'] = {\"gmm\": gmm_time, \"refine\": refine_time, \"init\":init_time, \"sampling\":sampling_time}\n",
        "  return total_accepted/f_eval_total, cur_samples, cur_samples_f, rej_samples, rej_samples_f, (ùúá, Œ£, weights), history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7DrN8u2udYN"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDo5XIFg0hYN"
      },
      "outputs": [],
      "source": [
        "results_Maddison = {}\n",
        "for a in tqdm([1, 5, 10, 15, 20], desc=\"Dimension Size\"):\n",
        "  results_Maddison[a] = []\n",
        "  for seed in tqdm(range(10), desc=\"Seed\", leave=False):\n",
        "    accpt_rate, samples, samples_f, r, r_f, g_, history = ers(jax.random.PRNGKey(seed), lambda x: log_f_Maddison(x, a), d=1,\n",
        "      target_samples=100000, samples_at_a_time=500, low_val=0, hi_val=jnp.inf)\n",
        "    results_Maddison[a].append(float(accpt_rate))\n",
        "\n",
        "  print(\"a: \", a)\n",
        "  print(\"\\t\", np.mean(results_Maddison[a]), np.std(results_Maddison[a]))\n",
        "  print(\"\\t\", results_Maddison[a])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxNbbQnQuaD-"
      },
      "outputs": [],
      "source": [
        "results_Erraqabi = {}\n",
        "for d in tqdm([1, 2, 3, 4, 5, 6, 7], desc=\"Dimension Size\"): #Testing 1-7\n",
        "  results_Erraqabi[d] = []\n",
        "  for seed in tqdm(range(10), desc=\"Seed\", leave=False):\n",
        "    accpt_rate, samples, samples_f, r, r_f, g_, history = ers(jax.random.PRNGKey(seed), lambda x: log_f_Erraqabi(x), d=d,\n",
        "      target_samples=100000, samples_at_a_time=500, low_val=0, hi_val=1)\n",
        "    results_Erraqabi[d].append(float(accpt_rate))\n",
        "\n",
        "  print(\"d: \", d)\n",
        "  print(\"\\t\", np.mean(results_Erraqabi[d]), np.std(results_Erraqabi[d]))\n",
        "  print(\"\\t\", results_Erraqabi[d])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8uZ4kEQ89X4"
      },
      "outputs": [],
      "source": [
        "results_clutter = {}\n",
        "for d in tqdm([1, 2], desc=\"Dimension Size\"): #Testing 1-2\n",
        "  results_clutter[d] = []\n",
        "  for seed in tqdm(range(10), desc=\"Seed\", leave=False):\n",
        "\n",
        "    sigma=2\n",
        "    pi = 0.5 # https://github.com/cmaddis/astar-sampling/tree/c65c5ff4cd779d5b528500f14428c4f216e0482c/examples\n",
        "    num_points = 10\n",
        "    points = np.concatenate((np.linspace(-5, -3, num_points), np.linspace(2, 4, num_points)))\n",
        "    data = np.zeros((len(points), d))\n",
        "    for q in range(d):\n",
        "      data[:,q] = points\n",
        "\n",
        "    data = jnp.array(data)\n",
        "    clutter = lambda x: log_f_clutter(x,data)\n",
        "    break\n",
        "    accpt_rate, samples, samples_f, r, r_f, g_, h = ers(jax.random.PRNGKey(seed), lambda x: clutter(x), d=d,\n",
        "      target_samples=100000, samples_at_a_time=500)\n",
        "    results_clutter[d].append(float(accpt_rate))\n",
        "\n",
        "  print(\"d:\")\n",
        "  print(\"\\t\", np.mean(results_clutter[d]), np.std(results_clutter[d]))\n",
        "  print(\"\\t\", results_clutter[d])\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}